import time
import hashlib
import json
import os
import requests
import re
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
import pytz # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å¯¾å¿œã®ãŸã‚

# --- ç’°å¢ƒè¨­å®š ---
# Supabase URLã¨APIã‚­ãƒ¼ã€‚ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãŸã‚ã€æœ¬ç•ªç’°å¢ƒã§ã¯ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚
SUPABASE_URL = "https://oahnmyfalnhdkoihrswh.supabase.co"
SUPABASE_API_KEY = os.environ.get("SUPABASE_API_KEY", "dummy")
SUPABASE_TABLE_NAME = "parking_cafe_reunion_arashiyama" # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¿å­˜ã™ã‚‹Supabaseãƒ†ãƒ¼ãƒ–ãƒ«å
STORE_ID = "Parking Cafe ReUnion Arashiyama" # å¯¾è±¡åº—èˆ—ã®ID

# Supabase APIãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã®å…±é€šãƒ˜ãƒƒãƒ€ãƒ¼
HEADERS = {
    "apikey": SUPABASE_API_KEY,
    "Authorization": f"Bearer {SUPABASE_API_KEY}",
    "Content-Type": "application/json"
}

# --- è£œåŠ©é–¢æ•° ---

# ç¾åœ¨æ™‚åˆ»ã‚’JSTã®ISOå½¢å¼ã§å–å¾— (inserted_atç”¨)
def jst_now():
    return datetime.now(pytz.timezone("Asia/Tokyo")).isoformat()

# ãƒ†ã‚­ã‚¹ãƒˆã‚’å®‰å…¨ã«floatã«å¤‰æ›ã€‚å¤‰æ›ã§ããªã„å ´åˆã¯Noneã‚’è¿”ã™
def safe_float(text):
    try:
        # ç©ºæ–‡å­—åˆ—ã€ãƒã‚¤ãƒ•ãƒ³ã€å…¨è§’ãƒã‚¤ãƒ•ãƒ³ã¯Noneã«
        return float(text) if text not in ["", "-", "ãƒ¼"] else None
    except (ValueError, TypeError):
        # å¤‰æ›ã§ããªã„å ´åˆã¯None
        return None

# ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ä¸€æ„ãªã‚­ãƒ¼ã‚’ç”Ÿæˆ (SHA256ãƒãƒƒã‚·ãƒ¥)
# ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®å†’é ­50æ–‡å­—ã‚’ä½¿ç”¨ã—ã€ã‚¹ãƒšãƒ¼ã‚¹ãªã©ã‚’é™¤å»ã—ã¦ãƒãƒƒã‚·ãƒ¥ã®å®‰å®šæ€§ã‚’é«˜ã‚ã‚‹
def safe_generate_unique_key(channel, store_id, visit_date, name, review_text):
    cleaned_review = re.sub(r"\s+", "", (review_text or "").strip().lower())[:50]
    cleaned_name = (name or "").strip().lower()
    cleaned_date = (visit_date or "").strip() # visit_dateãŒNoneã®å ´åˆã«ç©ºæ–‡å­—åˆ—ã«ãªã‚‹ã‚ˆã†ã«
    
    base = f"{channel}_{store_id}_{cleaned_date}_{cleaned_name}_{cleaned_review}"
    return hashlib.sha256(base.encode("utf-8")).hexdigest()

# --- WebDriverè¨­å®š ---

# WebDriverã®åˆæœŸè¨­å®šï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã€è‡ªå‹•åˆ¶å¾¡å›é¿ãªã©ï¼‰
def setup_driver():
    options = Options()
    options.add_argument("--headless=new") # æœ€æ–°ã®ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
    options.add_argument("--disable-blink-features=AutomationControlled") # è‡ªå‹•åˆ¶å¾¡ã‚’æ¤œçŸ¥ã•ã›ãªã„ã‚ˆã†ã«ã™ã‚‹
    options.add_argument("--no-sandbox") # Dockerã‚„CI/CDç’°å¢ƒã§å¿…è¦ã«ãªã‚‹å ´åˆãŒã‚ã‚‹
    options.add_argument("--disable-dev-shm-usage") # Dockerã‚„CI/CDç’°å¢ƒã§å¿…è¦ã«ãªã‚‹å ´åˆãŒã‚ã‚‹
    options.add_argument("--window-size=1920,1080") # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’è¨­å®š
    options.add_argument("--incognito") # ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•
    
    # WebDriverManagerã§ChromeDriverã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»è¨­å®š
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)

# --- Supabase é–¢é€£é–¢æ•° ---

# æ—¢å­˜ã®unique_keyã‚’Supabaseã‹ã‚‰ä¸€æ‹¬å–å¾—ã—ã€ã‚»ãƒƒãƒˆã§è¿”ã™
def fetch_existing_unique_keys():
    print("ğŸ”„ Supabaseã‹ã‚‰æ—¢å­˜ã®unique_keyã‚’å–å¾—ä¸­...")
    response = requests.get(f"{SUPABASE_URL}/rest/v1/{SUPABASE_TABLE_NAME}?select=unique_key", headers=HEADERS)
    if response.status_code == 200:
        existing_keys = {x["unique_key"] for x in response.json()}
        print(f"âœ… æ—¢å­˜ã®unique_keyã‚’ {len(existing_keys)} ä»¶å–å¾—ã—ã¾ã—ãŸã€‚")
        return existing_keys
    else:
        print(f"âŒ æ—¢å­˜ã‚­ãƒ¼å–å¾—ã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}")
        return set() # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®ã‚»ãƒƒãƒˆã‚’è¿”ã™

# ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’Supabaseã«æŒ¿å…¥ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
def insert_reviews_to_supabase(reviews_data, existing_keys_set):
    inserted_count = 0
    if not reviews_data:
        print("ğŸ’¡ æŒ¿å…¥ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return 0

    print(f"â˜ï¸ Supabaseã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ä¸­ï¼ˆåˆè¨ˆ {len(reviews_data)} ä»¶ï¼‰...")
    for r in reviews_data:
        unique_key = r.get('unique_key')
        if not unique_key:
            print(f"âš ï¸ unique_keyãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {r.get('name', 'N/A')}")
            continue

        # æ—¢å­˜ã®ã‚­ãƒ¼ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã¦ã„ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
        if unique_key in existing_keys_set:
            # print(f"âš ï¸ é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {unique_key} - {r.get('channel', 'N/A')} - {r.get('name', '')} ({r.get('visit_date', '')})")
            continue

        try:
            response = requests.post(f"{SUPABASE_URL}/rest/v1/{SUPABASE_TABLE_NAME}", headers=HEADERS, json=r)
            if response.status_code in [200, 201]:
                inserted_count += 1
                existing_keys_set.add(unique_key) # æ–°ã—ãæŒ¿å…¥ã•ã‚ŒãŸã‚­ãƒ¼ã‚’ãƒ¡ãƒ¢ãƒªä¸Šã®ã‚»ãƒƒãƒˆã«è¿½åŠ 
                print(f"âœ… ä¿å­˜æˆåŠŸ: {r.get('channel', 'N/A')} - {r.get('name', '')} ({r.get('visit_date', '')})")
            else:
                print(f"âŒ Supabase POSTã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text} - ãƒ‡ãƒ¼ã‚¿unique_key: {unique_key}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e} - ãƒ‡ãƒ¼ã‚¿unique_key: {unique_key}")
    print(f"ğŸ“¦ Supabaseã«æ–°è¦ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ {inserted_count} ä»¶ä¿å­˜ã—ã¾ã—ãŸã€‚")
    return inserted_count

# --- Google Maps ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•° ---

# Google Mapsã®ã‚¯ãƒã‚³ãƒŸã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯
def click_google_review_tab(driver):
    try:
        review_tab = WebDriverWait(driver, 20).until(
            EC.element_to_be_clickable((By.XPATH, '//button[contains(@aria-label,"ã‚¯ãƒã‚³ãƒŸ")]'))
        )
        driver.execute_script("arguments[0].click();", review_tab) # JavaScriptã§ã‚¯ãƒªãƒƒã‚¯ã‚’è©¦ã¿ã‚‹
        time.sleep(2) # ã‚¯ãƒªãƒƒã‚¯å¾Œã®å¾…æ©Ÿæ™‚é–“ã‚’é•·ãã™ã‚‹
        print("âœ… Google Maps ã‚¯ãƒã‚³ãƒŸã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯å®Œäº†")
    except Exception as e:
        print(f"âŒ Google Maps ã‚¯ãƒã‚³ãƒŸã‚¿ãƒ–ã‚¯ãƒªãƒƒã‚¯å¤±æ•—: {e}")

# Google Mapsã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã€Œæ–°ã—ã„é †ã€ã§ä¸¦ã³æ›¿ãˆ
def click_google_sort_by_new(driver):
    try:
        # ã€Œä¸¦ã¹æ›¿ãˆã€ãƒœã‚¿ãƒ³ã®XPathã‚’ã‚ˆã‚Šå…·ä½“çš„ã«
        sort_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, '//button[.//span[text()="ä¸¦ã¹æ›¿ãˆ"]]'))
        )
        driver.execute_script("arguments[0].click();", sort_button)
        time.sleep(1) # ã‚¯ãƒªãƒƒã‚¯å¾Œã®å¾…æ©Ÿ

        # ã€Œæ–°ã—ã„é †ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ãŸã‚ã®JavaScript
        script = '''
            const elements = Array.from(document.querySelectorAll('div[role="menuitem"]'));
            const target = elements.find(el => el.textContent.trim() === 'æ–°ã—ã„é †');
            if (target) { target.click(); return 'clicked'; } else { return 'not found'; }
        '''
        result = driver.execute_script(script)
        print(f"ğŸ§­ Google Maps ä¸¦ã³æ›¿ãˆã€æ–°ã—ã„é †ã€ã‚¯ãƒªãƒƒã‚¯çµæœ: {result}")
        time.sleep(2) # ä¸¦ã³æ›¿ãˆé©ç”¨å¾Œã®å¾…æ©Ÿæ™‚é–“ã‚’é•·ãã™ã‚‹
    except Exception as e:
        print(f"âŒ Google Maps ä¸¦ã³æ›¿ãˆã‚¯ãƒªãƒƒã‚¯å¤±æ•—: {e}")

# Google Mapsã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒªã‚¢ã‚’è¦‹ã¤ã‘ã‚‹
def find_google_scrollable_area(driver):
    print("ğŸ” Google Maps ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã‚’æ¢ç´¢ä¸­...")
    try:
        # ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã®æœ€åˆã®è¦ç´ ãŒå­˜åœ¨ã™ã‚‹ã¾ã§å¾…æ©Ÿã—ã€ãã®ç¥–å…ˆè¦ç´ ã‹ã‚‰ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªé ˜åŸŸã‚’æ¢ã™
        scrollable_div = WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-review-id]:first-of-type'))
        ).find_element(By.XPATH, './ancestor::div[contains(@class, "m6QErb") and contains(@class, "DxyBCb")]')
        print("âœ… Google Maps ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç™ºè¦‹")
        return scrollable_div
    except Exception as e:
        print(f"âŒ Google Maps ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {e}")
        return None

# Google Mapsã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æŒ‡å®šä»¶æ•°ã¾ãŸã¯æœ€å¾Œã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
def scroll_google_reviews(driver, scrollable_div, target_reviews=200, max_scrolls=200, patience_scrolls=10):
    prev_count = 0
    same_count_times = 0
    print(f"â†˜ï¸ Google Maps ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ (ç›®æ¨™: {target_reviews}ä»¶)...")
    for i in range(max_scrolls):
        driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight;", scrollable_div)
        time.sleep(0.5) # çŸ­ã‚ã®å¾…æ©Ÿ
        reviews = driver.find_elements(By.CSS_SELECTOR, 'div.jftiEf')
        curr_count = len(reviews)
        # print(f"   ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« {i+1}å›ç›®ï¼šç¾åœ¨ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•° = {curr_count}") # ãƒ‡ãƒãƒƒã‚°ç”¨

        if curr_count >= target_reviews:
            print(f"âœ… Google Maps {target_reviews}ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«åˆ°é” â†’ ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«çµ‚äº†")
            break
        if curr_count == prev_count:
            same_count_times += 1
            if same_count_times >= patience_scrolls:
                print(f"âš ï¸ Google Maps {patience_scrolls}å›é€£ç¶šã§ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°å¢—åŠ ãªã— â†’ ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­æ–­")
                break
        else:
            same_count_times = 0
            prev_count = curr_count
    
    # å…¨ã¦ã®ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å…¨æ–‡ã‚’è¡¨ç¤º
    try:
        expand_buttons = driver.find_elements(By.CSS_SELECTOR, 'button.w8nwRe')
        print(f"ğŸ‘ï¸â€ğŸ—¨ï¸ Google Maps ã€ã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ {len(expand_buttons)} å€‹ã‚’æ¤œå‡ºã€ã‚¯ãƒªãƒƒã‚¯ä¸­...")
        for btn in expand_buttons:
            try:
                driver.execute_script("arguments[0].click();", btn)
                # time.sleep(0.05) # çŸ­ã„å¾…æ©Ÿã§è² è·ã‚’æ¸›ã‚‰ã™
            except Exception as e:
                pass # å¤§é‡ã«å‡ºåŠ›ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯è¡¨ç¤ºã—ãªã„
        print("âœ… Google Maps ã€ã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯å®Œäº†")
    except Exception as e:
        print(f"âŒ Google Maps ã€ã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯å‡¦ç†å¤±æ•—: {e}")

# Google Mapsã‹ã‚‰ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã‚’æŠ½å‡º
def extract_google_reviews(driver):
    ref_date = datetime.now()  # åŸºæº–æ—¥ï¼ˆç¾åœ¨æ—¥æ™‚ï¼‰
    reviews = []

    # å®Œå…¨ã«å±•é–‹ã•ã‚ŒãŸãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—
    blocks = driver.find_elements(By.CSS_SELECTOR, 'div.jftiEf')
    print(f"âš™ï¸ Google Maps ãƒšãƒ¼ã‚¸ã‹ã‚‰ {len(blocks)} ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡ºä¸­...")

    for block in blocks:
        unique_key = block.get_attribute("data-review-id")
        if not unique_key:
            continue  # IDãŒãªã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯ã‚¹ã‚­ãƒƒãƒ—

        soup = BeautifulSoup(block.get_attribute("innerHTML"), "html.parser")

        review_text_elem = soup.select_one('.MyEned span.wiI7pd')
        review_text = review_text_elem.text.strip() if review_text_elem else ""
        if not review_text:
            continue  # æœ¬æ–‡ãªã—ã¯ã‚¹ã‚­ãƒƒãƒ—

        # æŠ•ç¨¿æ—¥ï¼ˆç›¸å¯¾è¡¨ç¾ or çµ¶å¯¾è¡¨ç¾ï¼‰
        visit_date = None
        visit_month = None
        try:
            rating_block = soup.select_one("div.DU9Pgb")
            if rating_block:
                publish_span = rating_block.select_one("span.rsqaWe")
                if publish_span:
                    txt = publish_span.get_text(strip=True)
                    visit_datetime = None

                    if "æ™‚é–“å‰" in txt:
                        hours = int(re.search(r"\d+", txt).group(0))
                        visit_datetime = ref_date - timedelta(hours=hours)
                    elif "æ—¥å‰" in txt:
                        days = int(re.search(r"\d+", txt).group(0))
                        visit_datetime = ref_date - timedelta(days=days)
                    elif "é€±é–“å‰" in txt:
                        weeks = int(re.search(r"\d+", txt).group(0))
                        visit_datetime = ref_date - timedelta(weeks=weeks * 7)
                    elif "ã‹æœˆå‰" in txt:
                        months = int(re.search(r"\d+", txt).group(0))
                        visit_datetime = ref_date - timedelta(days=30 * months)
                    elif "å¹´å‰" in txt:
                        years = int(re.search(r"\d+", txt).group(0))
                        visit_datetime = ref_date - timedelta(days=365 * years)
                    elif re.match(r"^\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥$", txt):
                        visit_datetime = datetime.strptime(txt, "%Yå¹´%mæœˆ%dæ—¥")
                    elif re.match(r"^\d{4}å¹´\d{1,2}æœˆ$", txt):
                        visit_datetime = datetime.strptime(txt, "%Yå¹´%mæœˆ")

                    if visit_datetime:
                        visit_date = visit_datetime.date().isoformat()
                        visit_month = visit_datetime.replace(day=1).date().isoformat()
        except Exception as e:
            print(f"âš ï¸ Google Maps visit_date æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e} (å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {txt if 'txt' in locals() else 'N/A'})")

        # ç·åˆã‚¹ã‚³ã‚¢
        total_score = None
        score_elem = soup.select_one('span.kvMYJc[aria-label]')
        if score_elem and "ã¤æ˜Ÿ" in score_elem.get("aria-label", ""):
            match = re.search(r"(\d+) ã¤æ˜Ÿ", score_elem['aria-label'])
            if match:
                total_score = int(match.group(1))

        # ã‚µãƒ–ã‚¹ã‚³ã‚¢æŠ½å‡ºãƒ˜ãƒ«ãƒ‘ãƒ¼
        def extract_google_sub_score(label):
            score = None
            score_blocks = soup.select("div.PBK6be span.RfDO5c")
            for block in score_blocks:
                bold = block.find("b")
                if bold and label in bold.text:
                    text = block.get_text(strip=True)
                    match = re.search(rf"{label}:\s*(\d+)", text)
                    if match:
                        score = int(match.group(1))
                        break
            return score

        food_score = extract_google_sub_score("é£Ÿäº‹")
        service_score = extract_google_sub_score("ã‚µãƒ¼ãƒ“ã‚¹")
        atmosphere_score = extract_google_sub_score("é›°å›²æ°—")

        lunch = "ãƒ©ãƒ³ãƒ" in review_text
        dinner = "ãƒ‡ã‚£ãƒŠãƒ¼" in review_text

        price_range = None
        price_elem = soup.find("span", attrs={"aria-label": re.compile(r"ï¿¥")})
        if price_elem:
            price_range = price_elem['aria-label']

        user_button = soup.select_one('button[aria-label*="ã•ã‚“ã®ã‚¯ãƒã‚³ãƒŸ"]')
        user_name = user_button['aria-label'].split('ã•ã‚“')[0] if user_button else ""
        a_tag = soup.select_one('a')
        user_url = "https://www.google.com" + a_tag['href'] if a_tag else ""

        reviews.append({
            "store_id": STORE_ID,
            "channel": "googlemaps",
            "unique_key": unique_key,
            "review": review_text,
            "visit_date": visit_date,
            "visit_month": visit_month,
            "name": user_name,
            "user_url": user_url,
            "total_score": total_score,
            "food_score": food_score,
            "service_score": service_score,
            "atmosphere_score": atmosphere_score,
            "lunch": lunch,
            "dinner": dinner,
            "price_range": price_range,
            "inserted_at": jst_now()
        })

    print(f"ğŸ“¦ Google Mapsã‹ã‚‰ {len(reviews)} ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æŠ½å‡ºå®Œäº†")
    return reviews

# ============ ãƒ¡ã‚¤ãƒ³å‡¦ç† ============

if __name__ == "__main__":
    # å„ã‚µã‚¤ãƒˆã®URLã‚’è¨­å®š
    # Google Maps URLã¯ã€ç›´æ¥Google Mapsã®åº—èˆ—ãƒšãƒ¼ã‚¸URLã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
    # ä¾‹: "https://www.google.com/maps/place/åº—èˆ—å/@ç·¯åº¦,çµŒåº¦,ã‚ºãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«z/data=!3m1!4b1!4m8!3m7!1s..."
    # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ–ãŒè‡ªå‹•ã§é–‹ãURLã¯ã€æœ«å°¾ã« /reviews ã‚’è¿½åŠ ã™ã‚‹ã¨è‰¯ã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
    google_maps_url = "https://www.google.com/maps/place/Parking+Cafe+ReUnion+Arashiyama/@35.0185282,135.6699421,16z/data=!4m12!1m2!2m1!1sParking+Cafe+ReUnion+Arashiyama!3m8!1s0x6001a97cb260f96d:0x8b89bd9d4be74cbb!8m2!3d35.0185282!4d135.6794693!9m1!1b1!15sCh9QYXJraW5nIENhZmUgUmVVbmlvbiBBcmFzaGl5YW1hWiEiH3BhcmtpbmcgY2FmZSByZXVuaW9uIGFyYXNoaXlhbWGSAQRjYWZlqgFoCg0vZy8xMXZ5d3QxNGcxEAEqECIMcGFya2luZyBjYWZlKAAyHhABIhpdOq0vpJ3wfvgVhz-YkgZsjwej9T-i4uwheTIjEAIiH3BhcmtpbmcgY2FmZSByZXVuaW9uIGFyYXNoaXlhbWHgAQA!16s%2Fg%2F11vywt14g1?entry=ttu&g_ep=EgoyMDI1MDUyMS4wIKXMDSoASAFQAw%3D%3D"

    driver = None # driverã‚’åˆæœŸåŒ–
    all_reviews = [] # å…¨ã‚µã‚¤ãƒˆã‹ã‚‰å–å¾—ã—ãŸãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ

    try:
        driver = setup_driver() # WebDriverã‚’ä¸€åº¦ã ã‘åˆæœŸåŒ–

        # æ—¢å­˜ã®unique_keyã‚’Supabaseã‹ã‚‰ä¸€åº¦ã ã‘å–å¾—
        existing_unique_keys = fetch_existing_unique_keys()

        # --- å„ã‚µã‚¤ãƒˆã‹ã‚‰ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—ã¨çµåˆ ---

        # Google Maps
        print("\n=== Google Maps ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—é–‹å§‹ ===")
        driver.get(google_maps_url)
        time.sleep(5) # ãƒšãƒ¼ã‚¸ãŒå®Œå…¨ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã®ã‚’å¾…ã¤
        click_google_review_tab(driver)
        click_google_sort_by_new(driver)
        scroll_target = find_google_scrollable_area(driver)
        if scroll_target:
            scroll_google_reviews(driver, scroll_target, target_reviews=200, max_scrolls=200) # ã‚ˆã‚Šå¤šãã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è©¦ã¿ã‚‹
            google_reviews = extract_google_reviews(driver)
            all_reviews.extend(google_reviews)
        else:
            print("âŒ Google Maps ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒªã‚¢ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        print("=== Google Maps ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—å®Œäº† ===\n")

        # Tabelog
        tabelog_reviews = scrape_tabelog(tabelog_url, STORE_ID, driver)
        all_reviews.extend(tabelog_reviews)

        # Hotpepper
        hotpepper_reviews = scrape_hotpepper(hotpepper_url, STORE_ID, driver)
        all_reviews.extend(hotpepper_reviews)

        # Ozmall
        ozmall_reviews = scrape_ozmall(ozmall_url, STORE_ID, driver)
        all_reviews.extend(ozmall_reviews)

        # Retty
        retty_reviews = scrape_retty(retty_url, STORE_ID, driver)
        all_reviews.extend(retty_reviews)

        # Ikyu
        ikyu_reviews = scrape_ikyu(ikyu_url, STORE_ID, driver)
        all_reviews.extend(ikyu_reviews)

    except Exception as e:
        print(f"è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    finally:
        if driver: # driverãŒNoneã§ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰çµ‚äº†
            driver.quit() # å…¨ã¦ã®å‡¦ç†ãŒçµ‚ã‚ã£ãŸã‚‰WebDriverã‚’é–‰ã˜ã‚‹

    print(f"\n--- å…¨ã‚µã‚¤ãƒˆåˆè¨ˆå–å¾—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°: {len(all_reviews)} ---\n")

    # --- å–å¾—ã—ãŸå…¨ã¦ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’Supabaseã«æŒ¿å…¥ ---
    if all_reviews:
        insert_reviews_to_supabase(all_reviews, existing_unique_keys)
    else:
        print("å–å¾—ã—ãŸãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚Supabaseã¸ã®æŒ¿å…¥ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
